{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chrisa142857/geom_tokenizer/blob/master/geom_token_nodelevel_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQdWAw1MkxS",
        "outputId": "01c3490b-cc10-40db-db47-e26cdb208f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=3d6897e62320612777fac7b902caab61460e32fd8f07cef923e80a682c33441c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, torch_geometric\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 torch_geometric-2.3.1 transformers-4.32.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ory8kH0GL2QT"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import trange, tqdm\n",
        "import transformers\n",
        "import random\n",
        "from datetime import datetime\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "def geom_tokenizer(node_feat: torch.Tensor, edge_index: torch.Tensor, N: int, dim: int=3):\n",
        "    nids = torch.arange(len(node_feat))\n",
        "    geom_tokens = []\n",
        "    token_count = []\n",
        "    dis_sorts = []\n",
        "    view_embeds = []\n",
        "    node_embeds = []\n",
        "    for ni in tqdm(nids, desc='Prepare tokens...'):\n",
        "        distances1 = ((node_feat[ni] - node_feat[ni+1:]) **2 ).sum(1)\n",
        "        if len(distances1) > 0:\n",
        "            mind = distances1.min()\n",
        "            maxd = distances1.max()\n",
        "        distances2 = ((node_feat[ni] - node_feat[:ni]) **2 ).sum(1)\n",
        "        if len(distances2) > 0:\n",
        "            mind = min(mind, distances2.min())\n",
        "            maxd = max(maxd, distances2.max())\n",
        "\n",
        "        distances = torch.cat([distances1, torch.FloatTensor([maxd+1]), distances2]) # X\n",
        "        ## spatially close nodes are neighborhood\n",
        "        dis_sort = distances.argsort()\n",
        "        dis_sorts.append(dis_sort)\n",
        "        nei_nid = dis_sort[:N]\n",
        "        ## connected nodes are neighborhood\n",
        "        connected_node = edge_index[1, edge_index[0] == ni]\n",
        "        nei_conn = (connected_node==nei_nid[..., None])\n",
        "        connected_node = connected_node[~(nei_conn.any(0))]\n",
        "        connected_node = connected_node[distances[connected_node].argsort()] # also sort connected nodes\n",
        "        nei_nid = torch.cat([nei_nid, connected_node]) # concat in the sorted rank\n",
        "        ## geom level 3, triangle, it has dim-1 = 2 view tokens\n",
        "        ## view token, max = N, the neighbor num\n",
        "        # view_id = torch.stack(torch.meshgrid(torch.arange(len(nei_nid)), torch.arange(len(nei_nid))), -1) # N x N x 2\n",
        "        # indices = torch.triu_indices(len(nei_nid), len(nei_nid), offset=1) # M = N x (N-1) / 2\n",
        "        # view_id = view_id[indices[0], indices[1]].T # 2 x M\n",
        "        view_id = torch.LongTensor(list(combinations(torch.arange(len(nei_nid)), dim-1))).T # dim-1 x M\n",
        "        # ## view nodes are sorted by distance\n",
        "        view_node = [nei_nid[view_id[di]] for di in range(dim-1)] # dim-1 x M, each is a node id\n",
        "        ## pos token, max = node num\n",
        "        # pos_token = torch.LongTensor([ni for _ in range(len(view_node[0]))]) # M\n",
        "        ## geom token, max = 2**3 = 8\n",
        "        nei_pair = torch.LongTensor(list(combinations(torch.arange(dim-1), 2))) # dim-1 * (dim-2) / 2 x 2, if neighbors connected\n",
        "        geom_token = torch.stack([torch.zeros_like(view_node[0]) for _ in range(dim-1+len(nei_pair))]) # dim x M\n",
        "        where_edges = torch.cat([nei_conn.any(1), torch.ones_like(connected_node, dtype=bool)])\n",
        "        # view_id[0] < view_id[1]\n",
        "        for di in range(dim-1):\n",
        "            where_edge = where_edges[view_id[di]]\n",
        "            geom_token[di, where_edge] = 1\n",
        "        ## if view points are connected\n",
        "        for di in range(len(nei_pair)):\n",
        "            where_edge = []\n",
        "            views = torch.stack([view_node[nei_pair[di, 0]], view_node[nei_pair[di, 1]]], -1)\n",
        "            for view in views: # for 2 in M x 2\n",
        "                where_edge.append((view == edge_index.T).any())\n",
        "            where_edge = torch.where(torch.stack(where_edge))[0]\n",
        "            geom_token[dim-1+di, where_edge] = 1\n",
        "        ## convert token to 10-base number\n",
        "        geom_token = bin2dec(geom_token) # M\n",
        "        geom_tokens.append(geom_token)\n",
        "        token_count.append(len(view_node[0]))\n",
        "        ## embed of geom direction from view points to cur node\n",
        "        view_embed = torch.stack([node_feat[view_node[di]] - node_feat[ni] for di in range(dim-1)]).sum(0) # M x C\n",
        "        view_embeds.append(view_embed)\n",
        "        node_embed = torch.stack([node_feat[ni] for _ in range(len(view_node[0]))]) # M x C\n",
        "        node_embeds.append(node_embed)\n",
        "\n",
        "    geom_tokens = torch.cat(geom_tokens)\n",
        "    dis_sorts = torch.stack(dis_sorts)\n",
        "    token_count = torch.FloatTensor(token_count)\n",
        "    view_dirs = torch.cat(view_embeds) #\n",
        "    node_embeds = torch.cat(node_embeds) #\n",
        "    # return pos_tokens, geom_tokens, view_tokens, node_embeds, token_count, dis_sorts\n",
        "    return geom_tokens, view_dirs, node_embeds, token_count, dis_sorts\n",
        "\n",
        "def geom_tokenizer_onenode(ni: int, node_feat: torch.Tensor, edge_index: torch.Tensor, N: int, dim: int=3):\n",
        "    distances1 = ((node_feat[ni] - node_feat[ni+1:]) **2 ).sum(1)\n",
        "    if len(distances1) > 0:\n",
        "        mind = distances1.min()\n",
        "        maxd = distances1.max()\n",
        "    distances2 = ((node_feat[ni] - node_feat[:ni]) **2 ).sum(1)\n",
        "    if len(distances2) > 0:\n",
        "        mind = min(mind, distances2.min())\n",
        "        maxd = max(maxd, distances2.max())\n",
        "\n",
        "    distances = torch.cat([distances1, torch.FloatTensor([maxd+1]), distances2]) # X\n",
        "    ## spatially close nodes are neighborhood\n",
        "    dis_sort = distances.argsort()\n",
        "    # dis_sorts.append(dis_sort)\n",
        "    nei_nid = dis_sort[:N]\n",
        "    ## connected nodes are neighborhood\n",
        "    connected_node = edge_index[1, edge_index[0] == ni]\n",
        "    nei_conn = (connected_node==nei_nid[..., None])\n",
        "    connected_node = connected_node[~(nei_conn.any(0))]\n",
        "    connected_node = connected_node[distances[connected_node].argsort()[:N]] # also sort connected nodes\n",
        "    nei_nid = torch.cat([nei_nid, connected_node]) # concat in the sorted rank\n",
        "    ## view token, max = N, the neighbor num\n",
        "    view_id = torch.LongTensor(list(combinations(torch.arange(len(nei_nid)), dim-1))).T # dim-1 x M\n",
        "    # ## view nodes are sorted by distance\n",
        "    view_node = [nei_nid[view_id[di]] for di in range(dim-1)] # dim-1 x M, each is a node id\n",
        "    ## geom token, max = 2**3 = 8\n",
        "    nei_pair = torch.LongTensor(list(combinations(torch.arange(dim-1), 2))) # dim-1 * (dim-2) / 2 x 2, if neighbors connected\n",
        "    geom_token = torch.stack([torch.zeros_like(view_node[0]) for _ in range(dim-1+len(nei_pair))]) # dim x M\n",
        "    where_edges = torch.cat([nei_conn.any(1), torch.ones_like(connected_node, dtype=bool)])\n",
        "    # view_id[0] < view_id[1]\n",
        "    for di in range(dim-1):\n",
        "        where_edge = where_edges[view_id[di]]\n",
        "        geom_token[di, where_edge] = 1\n",
        "    ## if view points are connected\n",
        "    for di in range(len(nei_pair)):\n",
        "        where_edge = []\n",
        "        views = torch.stack([view_node[nei_pair[di, 0]], view_node[nei_pair[di, 1]]], -1) # pair of views\n",
        "        for view in views: # for 2 in M x 2\n",
        "            where_edge.append((view == edge_index.T).any())\n",
        "        where_edge = torch.where(torch.stack(where_edge))[0]\n",
        "        geom_token[dim-1+di, where_edge] = 1\n",
        "    ## convert token to 10-base number\n",
        "    geom_token = bin2dec(geom_token) # M\n",
        "    token_count = torch.FloatTensor([len(view_node[0])])\n",
        "    ## embed of geom direction from view points to cur node\n",
        "    view_dir = torch.stack([node_feat[view_node[di]] - node_feat[ni] for di in range(dim-1)]).sum(0) # M x C\n",
        "    node_embed = torch.stack([node_feat[ni] for _ in range(len(view_node[0]))]) # M x C\n",
        "\n",
        "    return geom_token, view_dir, node_embed, token_count, dis_sort\n",
        "\n",
        "def bin2dec(b):\n",
        "    bits, batch = b.shape\n",
        "    mask = 2 ** torch.arange(bits - 1, -1, -1).to(b.device, b.dtype)\n",
        "    mask = torch.stack([mask for _ in range(batch)], 1)\n",
        "    return torch.sum(mask * b, 0)\n",
        "\n",
        "def token_zeropad(tokens, token_count, seq_len, datay, *args):\n",
        "    '''\n",
        "    tokens: M\n",
        "    M = token number\n",
        "    # n = token type number\n",
        "    '''\n",
        "    batches = []\n",
        "    labels = []\n",
        "    masks = [] # mask for zero padding\n",
        "    token_cumsum = torch.cumsum(token_count, 0)\n",
        "    token_cumsum = token_cumsum.long()\n",
        "    for i in range(len(token_cumsum)):\n",
        "        prev = 0 if i == 0 else token_cumsum[i-1]\n",
        "        seq = tokens[prev:token_cumsum[i].item()]\n",
        "        seq = seq[:seq_len]\n",
        "        mask = torch.ones(seq_len, dtype=bool, device=seq.device)\n",
        "        if len(seq) < seq_len:\n",
        "            if len(seq.shape) > 1:\n",
        "              one = torch.cat([seq, torch.zeros((seq_len-len(seq), seq.shape[1]), device=seq.device, dtype=seq.dtype)])\n",
        "            else:\n",
        "              one = torch.cat([seq, torch.zeros(seq_len-len(seq), device=seq.device, dtype=seq.dtype)])\n",
        "            mask[len(seq):] = False\n",
        "        else:\n",
        "            one = seq\n",
        "        labels.append(datay[i])\n",
        "        batches.append(one)\n",
        "        masks.append(mask)\n",
        "    batches = torch.stack(batches)\n",
        "    masks = torch.stack(masks)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    return batches, masks, labels\n",
        "\n",
        "def token_neighborpad(tokens, token_count, seq_len, datay, dis_sorts):\n",
        "    '''\n",
        "    tokens: M\n",
        "    M = token number\n",
        "    # n = token type number\n",
        "    '''\n",
        "    batches = []\n",
        "    labels = []\n",
        "    masks = [] # mask for class token\n",
        "    token_count = token_count.long()\n",
        "    token_cumsum = torch.cumsum(token_count, 0)\n",
        "    for i in range(len(token_count)):\n",
        "        prev = 0 if i == 0 else token_count[i-1]\n",
        "        seq = tokens[prev:prev+token_count[i]]\n",
        "        mask = torch.ones(seq_len, dtype=bool, device=seq.device)\n",
        "        if len(seq) < seq_len:\n",
        "            pid = 0\n",
        "            pcumsum = torch.cumsum(token_count[dis_sorts[i]], 0)\n",
        "            while pcumsum[pid] < seq_len-len(seq): pid += 1\n",
        "            pad_nid = dis_sorts[i, :pid]\n",
        "            for ni in pad_nid:\n",
        "                prev = token_cumsum[ni-1] if ni != 0 else 0\n",
        "                assert token_cumsum[ni] == prev + token_count[ni]\n",
        "                seq = torch.cat([seq, tokens[prev:token_cumsum[ni]]])\n",
        "        if len(seq) < seq_len:\n",
        "            if len(seq.shape) > 1:\n",
        "              seq = torch.cat([seq, torch.zeros((seq_len-len(seq), seq.shape[1]), device=seq.device, dtype=seq.dtype)])\n",
        "            else:\n",
        "              seq = torch.cat([seq, torch.zeros(seq_len-len(seq), device=seq.device, dtype=seq.dtype)])\n",
        "            mask[len(seq):] = False\n",
        "        one = seq[:seq_len]\n",
        "        labels.append(datay[i])\n",
        "        batches.append(one)\n",
        "        masks.append(mask)\n",
        "    batches = torch.stack(batches)\n",
        "    masks = torch.stack(masks)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    return batches, masks, labels\n",
        "\n",
        "class ToyModel(torch.nn.Module):\n",
        "    def __init__(self, node_num, node_channel, geom_dim, cls_num, nhead=8) -> None:\n",
        "        '''\n",
        "        Toy transformer for node classification\n",
        "        '''\n",
        "        super().__init__()\n",
        "        ## Embed all tokens\n",
        "        # self.encoder = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.encoder = transformers.BertModel(transformers.BertConfig())\n",
        "        # self.encoder.config.output_attentions = True\n",
        "        hdim = self.encoder.config.hidden_size\n",
        "        tokens_num = 1\n",
        "        # token_embed1 = torch.nn.Embedding(node_num, hdim//tokens_num)\n",
        "        token_embed2 = torch.nn.Embedding(2**geom_dim, hdim//tokens_num)\n",
        "        token_embed3 = torch.nn.Linear(node_channel, hdim//tokens_num)\n",
        "        # token_embed3 = torch.nn.Embedding(node_num**2, hdim//tokens_num)\n",
        "        # token_embed4 = torch.nn.Embedding(node_num, hdim//4)\n",
        "        self.token_embeds = torch.nn.ModuleList([token_embed2, token_embed3]) #, token_embed4\n",
        "        self.node_embed =  torch.nn.Linear(node_channel, hdim) #, token_embed4\n",
        "        ## Transformer Encoder\n",
        "        # encoder_layer = torch.nn.TransformerEncoderLayer(d_model=hdim, nhead=nhead)\n",
        "        # self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.classifier = torch.nn.Linear(hdim, cls_num)\n",
        "\n",
        "    def forward(self, inputs, masks=None):\n",
        "        # x, pos_tokens, geom_tokens, view_tokens = inputs\n",
        "        ## geom_token + view_token\n",
        "        embeds = []\n",
        "        for f, token in zip(self.token_embeds, inputs[1:]):\n",
        "           embeds.append(f(token))\n",
        "        embeds = torch.stack(embeds, 0).sum(0)\n",
        "        ## geom_token + view_token + node feat\n",
        "        # embeds = embeds + self.node_embed(inputs[0])\n",
        "        ## node feat\n",
        "        # embeds = self.node_embed(inputs[0])\n",
        "        outputs = self.encoder(inputs_embeds=embeds, attention_mask=masks)\n",
        "        ## last_hidden_state, pooler_output, attentions = outputs\n",
        "        out = self.classifier(outputs[1])\n",
        "        return out\n",
        "\n",
        "def toy_trainval(batches_list, data_idx, train=True, use_mask=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    losses = []\n",
        "    preds = []\n",
        "    idx_shuffle = list(range(0, len(data_idx), batch_size))\n",
        "    id_list = []\n",
        "    random.shuffle(idx_shuffle)\n",
        "    for bi, i in enumerate(idx_shuffle):\n",
        "        idx = data_idx[i:i+batch_size]\n",
        "        id_list.append(idx)\n",
        "        batch = [batches[idx].to(device) for batches in batches_list]\n",
        "        mask = masks[idx].to(device)\n",
        "        label = labels[idx].to(device)\n",
        "        if train:\n",
        "            if use_mask:\n",
        "              out = model(batch, mask)\n",
        "            else:\n",
        "              out = model(batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(out, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                if use_mask:\n",
        "                  out = model(batch, mask)\n",
        "                else:\n",
        "                  out = model(batch)\n",
        "            loss = loss_fn(out, label)\n",
        "        pred = out.max(1)[1].detach().cpu()\n",
        "        preds.append(pred)\n",
        "        losses.append(loss.detach().cpu())\n",
        "    preds = torch.cat(preds)\n",
        "    losses = torch.stack(losses)\n",
        "    id_list = torch.cat(id_list)\n",
        "    acc = preds.eq(labels[id_list]).sum().item() / len(data_idx)\n",
        "    return losses.mean().item(), acc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def binary(x, bits):\n",
        "    mask = 2**torch.arange(bits-1,-1,-1).to(x.device, x.dtype)\n",
        "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte()\n",
        "\n",
        "class DataBatchSet(Dataset):\n",
        "\n",
        "    def __init__(self, node_feat, edge_index, label, node_idx=None, mask=None, node_feat2bin=False, N=10, geom_dim=3, seq_len=512) -> None:\n",
        "        self.node_feat = node_feat\n",
        "        self.edge_index = edge_index\n",
        "        self.label = label\n",
        "        assert len(node_feat) == len(label)\n",
        "        if isinstance(node_feat, list):\n",
        "            ## if task is graph level, sequence will include all nodes of a graph\n",
        "            self.graph_level = True\n",
        "            self.node_idx = []\n",
        "            for gi in range(len(node_feat)):\n",
        "                for ni in range(len(node_feat[gi])):\n",
        "                    self.node_idx.append([gi, ni])\n",
        "            self.node_idx = torch.LongTensor(self.node_idx)\n",
        "        else:\n",
        "            ## if task is node level, sequence will include tokens of one node\n",
        "            self.graph_level = False\n",
        "            if node_idx is not None:\n",
        "                self.node_idx = node_idx\n",
        "            elif mask is not None:\n",
        "                self.node_idx = torch.where(mask)[0]\n",
        "            else:\n",
        "                self.node_idx = torch.arange(len(node_feat))\n",
        "        self.N = N\n",
        "        self.dim = geom_dim\n",
        "        self.seq_len = seq_len\n",
        "        self.node_feat2bin = node_feat2bin\n",
        "        if node_feat2bin:\n",
        "            self.node_feat_ch = len(bin(max([f.max() for f in node_feat]))) - 2\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        if self.graph_level:\n",
        "            gi, ni = self.node_idx[i]\n",
        "            node_feat = self.node_feat[gi]\n",
        "            if self.node_feat2bin:\n",
        "                node_feat = binary(node_feat, self.node_feat_ch)\n",
        "            edge_index = self.edge_index[gi]\n",
        "            datay = self.label[gi]\n",
        "        else:\n",
        "            ni = self.node_idx[i]\n",
        "            node_feat = self.node_feat\n",
        "            edge_index = self.edge_index\n",
        "            datay = self.label[ni:ni+1]\n",
        "\n",
        "        geom_tokens, view_dirs, node_embeds, token_count, distance_sorts = geom_tokenizer_onenode(ni, node_feat, edge_index, self.N, self.dim)\n",
        "        geom_tokens, masks, labels = token_zeropad(geom_tokens, token_count, self.seq_len, datay, distance_sorts)\n",
        "        # geom_batches_d4, masks_d4, _ = token_padder(geom_tokens_d4, token_d4_count, seq_len, data.y, distance_sorts)\n",
        "        view_dirs, _, _ = token_zeropad(view_dirs, token_count, self.seq_len, datay, distance_sorts)\n",
        "        # view_batches_d4, _, _ = token_padder(view_dirs_d4, token_d4_count, seq_len, data.y, distance_sorts)\n",
        "        node_embeds, _, _ = token_zeropad(node_embeds, token_count, self.seq_len, datay, distance_sorts)\n",
        "        return node_embeds[0], geom_tokens[0], view_dirs[0], masks[0], labels[0], gi if self.graph_level else ni\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.node_idx)"
      ],
      "metadata": {
        "id": "AfJ2F4GUbiS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import WebKB, WikipediaNetwork, Actor, ZINC, AQSOL, WikiCS, GNNBenchmarkDataset, Planetoid\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_data_pyg(name, split=0):\n",
        "  path = '../data/' +name\n",
        "  if name in ['chameleon','squirrel']:\n",
        "    dataset = WikipediaNetwork(root=path, name=name)\n",
        "  if name in ['cornell', 'texas', 'wisconsin']:\n",
        "    dataset = WebKB(path ,name=name)\n",
        "  if name == 'film':\n",
        "    dataset = Actor(root=path)\n",
        "  if name == 'zinc':\n",
        "    dataset = ZINC(root=path)\n",
        "  if name in ['pubmed', 'cora', 'citeseer']:\n",
        "    dataset = Planetoid(root=path, name=name, split='geom-gcn')\n",
        "\n",
        "  if name in ['pubmed', 'cora', 'citeseer']:\n",
        "    data = dataset\n",
        "    data.train_mask = data.train_mask[:, split]\n",
        "    data.val_mask = data.val_mask[:, split]\n",
        "    data.test_mask = data.test_mask[:, split]\n",
        "  else:\n",
        "    data = dataset[0]\n",
        "    if name in ['chameleon', 'squirrel']:\n",
        "      splits_file = np.load(f'{path}/{name}/geom_gcn/raw/{name}_split_0.6_0.2_{split}.npz')\n",
        "    if name in ['cornell', 'texas', 'wisconsin']:\n",
        "      splits_file = np.load(f'{path}/{name}/raw/{name}_split_0.6_0.2_{split}.npz')\n",
        "    if name == 'film':\n",
        "      splits_file = np.load(f'{path}/raw/{name}_split_0.6_0.2_{split}.npz')\n",
        "    if name in ['Cora', 'Citeseer', 'Pubmed']:\n",
        "        splits_file = np.load(f'{path}/{name}/raw/{name}_split_0.6_0.2_{split}.npz')\n",
        "    train_mask = splits_file['train_mask']\n",
        "    val_mask = splits_file['val_mask']\n",
        "    test_mask = splits_file['test_mask']\n",
        "\n",
        "    data.train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
        "    data.val_mask = torch.tensor(val_mask, dtype=torch.bool)\n",
        "    data.test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
        "\n",
        "  return data\n"
      ],
      "metadata": {
        "id": "-pa5Ae9jMgur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def trainval(loader, train=True, use_mask=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "        loop_type = 'train'\n",
        "    else:\n",
        "        model.eval()\n",
        "        loop_type = 'val/test'\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    for data in tqdm(loader, desc=f'Epoch [{e+1}\\t/{epoch}]'):\n",
        "        pad_mask_d3, label, did = data[3:]\n",
        "        batch = [input.to(device) for input in data[:3]]\n",
        "        mask = pad_mask_d3.to(device)\n",
        "        label = label.to(device)\n",
        "        if train:\n",
        "            if use_mask:\n",
        "              out = model(batch, mask)\n",
        "            else:\n",
        "              out = model(batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(out, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                if use_mask:\n",
        "                  out = model(batch, mask)\n",
        "                else:\n",
        "                  out = model(batch)\n",
        "            loss = loss_fn(out, label)\n",
        "        pred = out.max(1)[1].detach().cpu()\n",
        "        preds.append(pred)\n",
        "        labels.append(label.detach().cpu())\n",
        "        losses.append(loss.detach().cpu())\n",
        "    preds = torch.cat(preds)\n",
        "    preds = torch.cat(labels)\n",
        "    losses = torch.stack(losses)\n",
        "    id_list = torch.cat(id_list)\n",
        "    acc = preds.eq(labels).sum().item() / len(labels)\n",
        "    return losses.mean().item(), acc\n",
        "\n",
        "torch.manual_seed(142857)\n",
        "device = 'cuda:0'\n",
        "seq_len = 512\n",
        "batch_size = 16\n",
        "epoch = 100\n",
        "lr = 1e-6\n",
        "use_mask = True\n",
        "\n",
        "for i in range(10):\n",
        "    data = get_data_pyg('pubmed', split=i)\n",
        "    train_set = DataBatchSet(data.x, data.edge_index, data.y, mask=data.train_mask)\n",
        "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    val_set = DataBatchSet(data.x, data.edge_index, data.y, mask=data.val_mask)\n",
        "    valloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "    test_set = DataBatchSet(data.x, data.edge_index, data.y, mask=data.test_mask)\n",
        "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    model = ToyModel(0, data.x.shape[1], 3, data.y.max().item()+1).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(),lr=lr) # 1e-4,weight_decay=0.01\n",
        "    print(f'[start train val on split {i}]')\n",
        "    for e in range(epoch):\n",
        "        train_loss, train_acc = trainval(trainloader, train=True, use_mask=True)\n",
        "        val_loss, val_acc = trainval(valloader, train=False, use_mask=use_mask)\n",
        "        test_loss, test_acc = trainval(testloader, train=False, use_mask=use_mask)\n",
        "        log = f'Epoch [{e+1}\\t/{epoch}] Train Loss: {train_loss:.03f} \\t Train Acc: {train_acc:.06f} \\t Val Loss: {val_loss:.03f} \\t Val Acc: {val_acc:.06f} \\t Test Loss: {test_loss:.03f} \\t Test Acc: {test_acc:.06f}'\n",
        "        print(datetime.now(), log)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drqEvnqsbzDm",
        "outputId": "7fc8f0e8-8692-4b82-a5e6-b221ef42c1e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1\t/100]:  53%|█████▎    | 314/592 [12:48<11:11,  2.41s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(142857)\n",
        "# device = 'cuda:0'\n",
        "# seq_len = 512\n",
        "# batch_size = 16\n",
        "# epoch = 100\n",
        "# lr = 1e-6\n",
        "# token_padder = token_zeropad\n",
        "# # token_padder = token_neighborpad\n",
        "# use_mask = True\n",
        "# data = get_data_pyg('pubmed', split=0)\n",
        "# geom_tokens, view_dirs, node_embeds, token_count, distance_sorts = geom_tokenizer(data.x, data.edge_index, 10, dim=3)\n",
        "# # geom_tokens_d4, view_dirs, node_embeds, token_count, distance_sorts = geom_tokenizer(data.x, data.edge_index, 10, dim=4)\n",
        "# geom_batches, masks, labels = token_padder(geom_tokens, token_count, seq_len, data.y, distance_sorts)\n",
        "# view_batches, _, _ = token_padder(view_dirs, token_count, seq_len, data.y, distance_sorts)\n",
        "# x_batches, _, _ = token_padder(node_embeds, token_count, seq_len, data.y, distance_sorts)\n",
        "# for i in range(10):\n",
        "#     data = get_data_pyg('pubmed', split=i)\n",
        "#     train_idx = torch.where(data.train_mask)[0]\n",
        "#     val_idx = torch.where(data.val_mask)[0]\n",
        "#     test_idx = torch.where(data.test_mask)[0]\n",
        "#     loss_fn = torch.nn.CrossEntropyLoss()\n",
        "#     model = ToyModel(len(data.x), data.x.shape[1], 3, data.y.max().item()+1).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(),lr=lr) # 1e-4,weight_decay=0.01\n",
        "#     # optimizer = optim.SGD(model.parameters(),lr=lr) # 1e-3\n",
        "#     inputs = [x_batches, geom_batches, view_batches]\n",
        "#     # inputs = [data.x.unsqueeze(1)]\n",
        "#     print([i.shape for i in inputs])\n",
        "#     for e in range(epoch):\n",
        "#         train_loss, train_acc = toy_trainval(inputs, train_idx, train=True, use_mask=use_mask)\n",
        "#         val_loss, val_acc = toy_trainval(inputs, val_idx, train=False, use_mask=use_mask)\n",
        "#         test_loss, test_acc = toy_trainval(inputs, test_idx, train=False, use_mask=use_mask)\n",
        "#         log = f'Epoch [{e+1}\\t/{epoch}] Train Loss: {train_loss:.03f} \\t Train Acc: {train_acc:.06f} \\t Val Loss: {val_loss:.03f} \\t Val Acc: {val_acc:.06f} \\t Test Loss: {test_loss:.03f} \\t Test Acc: {test_acc:.06f}'\n",
        "#         print(datetime.now(), log)"
      ],
      "metadata": {
        "id": "suTiM4GTMZdX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}